---
title: 1.6 Linear
type: docs
---

*Converting contextual abstractions to vocabulary.*

![transformer-repeated](/img/transformer-vaswani.png)

*Diagram 1.0: The Transformer, Vaswani et al. (2017)*

The decoder will output a set of numerical vectors (one per token, including both the input sequence and the tokens generated so far) of a prespecified, computationally efficient dimension (e.g. 5120 is the default configuration, in the case of Llama-4). The linear layer, acting as a classifier, will convert each vector to a new vector, the size of the new vector being of the same size as the total known vocabulary of the model (potentially to \~120,000 words \- the size of a recent paperback dictionary).


![Linear layer of The Transformer, represented as neural network (SLP)](/img/linear-SLP.svg)

*Diagram 1.6.1: a vector representing a processed token (x<sub>1</sub>) of dimension 4 being run through an SLP of 5 neurons, meaning a model with a total known vocabulary of 5 words. Note that each neuron will be fed a different set of trainable weights.*

| Output variable | Variable value | Word | 
| :---- | :---- | :---- |
| \(p_{1}\) | 0.5 | Sunny |
| \(p_{2}\) | 1 | Cloudy |
| \(p_{3}\) | 2 | Rainy |
| \(p_{4}\) | 0.5 | Misty |
| \(p_{5}\) | 6 | Snowy |

The above table relates to the diagram 1.6.1 above. For example, if the input sequence was "today's weather?", and the LLM had learnt to make predictions related to weather such as from chat conversations and past weather data. Vector x<sub>1</sub> could have been generated by the Transformer, based on the token "weather?" being input to the Transformer, scores then assigned by the linear layer as to which upcoming tokens are most likely. Once the Transformer has been trained, there is an injective relation between each p<sub>i</sub> variable and every token that the model is able to generate.


The name linear is due to this stage consisting of a fully-connected linear layer of an SLP (without an activation function, or equivalently, an identity activation function). Recall that an SLP has no hidden layers \- the inputs are combined with weights to generate a set of outputs. As with all parameters, these weights are trained when the Transformer is trained in its entirety. The vectors output by the SLP will each consist of a set of floats.

Due to the simplicity of an SLP, the linear layer can alternatively be thought of as a matrix of trainable weights.