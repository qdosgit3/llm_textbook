---
title: 1.5 Feed-forward Network
type: docs
---

*Deductions based on training and utilisation input.*

<img src="/img/transformer-vaswani.png" alt="The Transformer" width="50%"/>

*Diagram 1: The Transformer, Vaswani et al. (2017)*

It is difficult to swiftly summarise the inner mechanisms of a neural network (a.k.a. feed forward network). These concepts are covered from the ground up in University of Manchester's COMP24112, which builds up the concept of the neural network in the following steps:

1. models \- linear regression models and the value of the weights within them  
2. training \- via a dataset setup for supervised learning, training a linear regression model by updating weights via gradient descent  
3. forward propagation \- connecting neurons and deriving inputs, via the outputs of previous neurons after being processed through activation functions, to form an MLP  
4. backpropagation \- training an MLP via backpropagation (multiplication of a series of partial derivatives to deduce exactly how a change in each weight affects the loss function) and gradient descent

The contents of COMP24112 are covered at: https://cs-notes.xza.fr

Essentially a neural network is a mathematical implementation of the mechanisms of the human brain, in that, during utilisation, outputs are derived, via inputs to a series of functions in the previous layer. These functions either manipulate the data, or pass the data onto the next layer within an MLP (or don't, if the output does not meet the conditions of the activation function deployed).

The introductory internals of how a neural network works are considered prerequisite knowledge for this text, and can be found already in resources such as Harvard's Undergraduate Fundamentals of Machine Learning textbook, or in Appendix A of LLM Deep Dive from Springer.

Specifically however, in the context of the LLM, the feed forward network:

* inputs the normalised self-attention outputs, one feed forward network per token  
* works independently on each token (unlike the self-attention mechanism, which slots them all into concurrent matrix operations), whilst using the newly embedded relational data to other tokens from the self-attention layer  
* the feed forward network within the encoder must output a data format compatible with the multi-head self-attention layer (i.e. a high dimensional vector representation of a token)  
* scales the dimensionality of the input upwards, and then back downwards

Scaling dimensionality upwards can make it easier to group data into appropriate patterns, which is vital during training the model to generate patterns, and then again during utilisation to match the pre-generated intricate patterns. Imagine transitioning from 2D co-ordinates to 3D co-ordinates to generate or match the shape of a river. This is essentially the purpose of the feed forward neural network in the context of an LLM \- finding the patterns in a text corpus during training, and then applying them to an input sequence during utilisation.
