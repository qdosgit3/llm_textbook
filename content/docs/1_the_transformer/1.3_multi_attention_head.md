---
title: 1.3 Multi-attention Head
type: docs
---

*Finding relations between tokens.*

<img src="/img/transformer-vaswani.png" alt="The Transformer" width="50%"/>

*Diagram 1: The Transformer, Vaswani et al. (2017)*

During utilisation, attention regards finding how strongly tokens are interrelated, based on specific types of relations.  
For example, in the context of translation, words cannot be translated word-by-word independently, due to differences in sentence structures across languages, such as English's subject-verb-object pattern compared to the subject-object-verb pattern found in other languages (e.g. Japanese). Instead, the grammatical relations between the words must first be understood.

Mathematically, a single self-attention mechanism head calculates attention scores as follows:  
\(attention(Q, K, V) = softmax( \frac {QK^{T}} {\sqrt {d_{k}} } + M) V \)

Note the use of the dot product, which mathematically tends to feature in vector manipulations, because the inputs (tokens) are essentially being represented as vectors (in matrix **Q**) in potentially a dimension outside of reality (e.g. 2048 dimensions), and then compared to the metadata of the other tokens (in matrix **K**) also stored as vectors of the same dimension. The result of the dot product will then indicate the best matching token meanings (represented as numbers, in matrix **V**).

This self-attention mechanism can be broken down into a step-by-step sequence:

1. All the input vectors x<sub>i</sub> (such as those output from the positional encoding step), are set into a matrix **X**; for example 6 input vectors of dimension 4 would be arranged as a 6x4 matrix.  


| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | <!-- --> | 
| :---- | :---- | :---- | :---- | :---- |
| \(x_{1}\) | 0.1 | -0.2 | 0.1 | 0.2 |
| \(x_{2}\) | 0.8 | 0.8 | -0.9 | 0.5 |
| \(x_{3}\) | -0.6 | 0.4 | 0.5 | -0.9 |
| \(x_{4}\) | 0.9 | -0.7 | 0.8 | 0.7 |
| \(x_{5}\) | 0.4 | 0.3 | 0.4 | 0.4 |
| \(x_{6}\) | 0.8 | 0.2 | 0.9 | -0.2 |

*Sample matrix **X** above could represent an input such as the sentence "List new ideas for a song", one word per **x**<sub>i</sub> row, in the simple case of 4 dimensions per token.*


2. The matrix **X** is multiplied by 3 weight matrices of equivalent dimensions (for the dimensions in this example: 4x4), one matrix **W<sup>Q</sup>** representing query weights, one matrix **W<sup>K</sup>** representing key weights, and one matrix **W<sup>V</sup>** representing value weights, to generate three new matrices **Q**, **K**, and **V**. 

**Q** = **XW<sup>Q</sup>**

**K** = **XW<sup>K</sup>**

**V** = **XW<sup>V</sup>**


| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | 
| :---- | :---- | :---- | :---- |
| 3.4 | -2.1 | 0.8 | 1.9 |
| -1.2 | 0.5 | 3.7 | -0.8 |
| 2.9 | 1.1 | -3.4 | 0.6 |
| 0.7 | 2.8 | 1.3 | -2.5 |

*Sample matrix **W<sup>Q</sup>** above.*

| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | 
| :---- | :---- | :---- | :---- | :---- |
| \(q_{1}\) | 1.01 | 0.36 | -0.74 | -0.09 |
| \(q_{2}\) | -0.50 | -0.87 | 4.49 | -0.23 |
| \(q_{3}\) | 2.17 | -1.99 | -3.92 | 1.71 |
| \(q_{4}\) | 1.43 | 2.59 | 1.97 | -3.19 |
| \(q_{5}\) | 0.67 | 0.98 | 0.61 | -0.31 |
| \(q_{6}\) | 1.97 | 0.52 | 4.01 | -0.51 |

*Sample matrix **Q** above.*

3. The dot product of each query vector **q**<sub>i</sub> (a query vector being a row of **Q**) with every key ki via transposed **KT** is calculated, to generate attention scores; \(\ QK^{T} \). 
  Higher attention scores between a given **q**<sub>i</sub> and a given **k**<sub>i</sub> indicaŧe that the query **q**<sub>i</sub> is more similar to the given **k**<sub>i</sub>, and therefore that there is a stronger relation. 

attention(**Q**, **K**) = **QK<sup>T</sup>**

| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | <!-- --> | 
| :---- | :---- | :---- | :---- | :---- | :---- |
| 0.15 | 1.94 | -0.98 | 2.53 | 1.24 | 2.18 |
| 0.16 | 1.44 | 0.64 | -2.33 | 0.81 | 0.44 |
| 0.25 | 0.35 | 1.25 | 2.24 | 1.14 | 3.25 |
| 0.28 | 1.15 | -0.81 | 1.93 | -0.81 | -0.28 |

*Sample matrix **K<sup>T</sup>** above.*


| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | <!-- --> | 
| :---- | :---- | :---- | :---- | :---- | :---- |
| -0.01 | 2.12 | -1.62 | 3.45 | 1.83 | 3.29 |
| -1.37 | 0.51 | 7.15 | -3.39 | 2.35 | 2.99 |
| 3.49 | -6.19 | -9.39 | 11.19 | -3.19 | -5.67 |
| 2.23 | 3.19 | 2.35 | -0.67 | 1.35 | 4.37 |
| 0.83 | 1.42 | 0.95 | 0.27 | 0.69 | 1.83 |
| 3.35 | 1.35 | 10.93 | -0.95 | 3.35 | 5.95 |

*Sample matrix **QK<sup>T</sup>** above.*

4. These attention scores are then normalised by dividing by the square root of the dimension being used (in this example, 4); \(\frac {QK^{T}}{\sqrt{d_{k}}}\).

| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | <!-- --> | 
| :---- | :---- | :---- | :---- | :---- | :---- |
| -0.005 | 1.06 | -0.81 | 1.725 | 0.915 | 1.645 |
| -0.685 | 0.255 | 3.575 | -1.695 | 1.175 | 1.495 |
| 1.745 | -3.095 | -4.695 | 5.595 | -1.595 | -2.835 |
| 1.115 | 1.595 | 1.175 | -0.335 | 0.675 | 2.185 |
| 0.415 | 0.71 | 0.475 | 0.135 | 0.345 | 0.915 |
| 1.675 | 0.675 | 5.465 | -0.475 | 1.675 | 2.975 |

*Sample matrix **QK<sup>T</sup>** above, where all cells have been divided by √4*.

5. In the decoder, a masking matrix **M** is used to force infinitely negative attention scores between tokens being queried and future tokens, such that only attention scores between a given token and tokens previously generated are used; \( \frac {QK^{T}}{\sqrt{d_{k}}} + M \).

| <!-- --> | <!-- --> | <!-- --> | <!-- --> | <!-- --> | <!-- --> | 
| :---- | :---- | :---- | :---- | :---- | :---- |
| 0 | \-∞ | \-∞ | \-∞ | \-∞ | \-∞ |
| 0 | 0 | \-∞ | \-∞ | \-∞ | \-∞ |
| 0 | 0 | 0 | \-∞ | \-∞ | \-∞ |
| 0 | 0 | 0 | 0 | \-∞ | \-∞ |
| 0 | 0 | 0 | 0 | 0 | \-∞ |
| 0 | 0 | 0 | 0 | 0 | 0 |

*Sample matrix **M** above.*

6. Softmax is run on the attention scores, once per column, nullifying the infinitely negative scores, and generating a probability distribution from the attention scores; \( softmax ( \frac {QK^{T}}{\sqrt{d_{k}}} +M) \).

| <!-- --> | <!-- --> | <!-- --> | <!-- --> | <!-- --> | <!-- --> | 
| :---- | :---- | :---- | :---- | :---- | :---- |
| 0.156 | 0 | 0 | 0 | 0 | 0 |
| 0.138 | 0.041 | 0 | 0 | 0 | 0 |
| 0.194 | 0.001 | 0 | 0 | 0 | 0 |
| 0.168 | 0.193 | 0.042 | 0.143 | 0 | 0 |
| 0.142 | 0.136 | 0.031 | 0.051 | 0.083 | 0 |
| 0.201 | 0.629 | 0.927 | 0.806 | 0.917 | 1 |

*Sample matrix after softmax() has been applied, note that the cells in each column add up to 1.*

7. The probability distribution is then applied to the values within **V** to find the input tokens with the largest influence on the token under focus; \( softmax( \frac {QK^{T}} {\sqrt {d_{k}} } + M) V \) \).

| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | 
| :---- | :---- | :---- | :---- |
| 0.43 | 0.29 | 0.64 | 0.83 |
| 1.15 | 1.19 | -0.28 | 0.67 |
| -0.21 | 0.64 | 0.59 | -0.82 |
| 1.19 | -0.45 | 0.86 | 0.99 |
| 0.51 | 0.41 | 0.51 | 0.51 |
| 1.19 | 0.28 | 0.95 | -0.15 |

*Sample matrix **V**.*

| <!-- --> |  <!-- --> |  <!-- --> |  <!-- --> | 
| :---- | :---- | :---- | :---- |
| 0.06708 | 0.04524 | 0.09984 | 0.12948 |
| 0.15874 | 0.15766 | 0.05938 | 0.08634 |
| 0.08382 | 0.05966 | 0.11566 | 0.13938 |
| 0.26734 | 0.06388 | 0.16438 | 0.20534 |
| 0.15538 | 0.13636 | 0.08656 | 0.09556 |
| 1.05279 | 0.68873 | 1.05273 | 0.82373 |

*Sample matrix of final output of one self-attention head; at this point in the Transformer, each token now has both positional data and data regarding relations to other tokens, embedded into one vector.*

Notes regarding the above example matrices:

* a matrix **W<sup>K</sup>** is of size 4x4 means that there are 4 keys stored within the self-attention mechanism, each with a level of detail of dimension 4 
* the cells have been initialised with random values, and this is how weights in a real Transformer are setup, and then trained via supervised learning
* in modern LLM implementations, the dimensions of the key, value, and initial inputs to the Transformer may all differ in dimensionality
* matrix multiplications were handled by Llama-4, in Markdown, and have not been thoroughly checked; this is just an example for illustrative purposes and there may be small numerical errors, though the dimensions are correct

More algebraic, implicit examples of the matrix transformations are available at:
- [https://www.columbia.edu/\~jsl2239/transformers.html](https://www.columbia.edu/\~jsl2239/transformers.html)
- [https://web.stanford.edu/\~jurafsky/slp3/9.pdf](https://web.stanford.edu/\~jurafsky/slp3/9.pdf)

The multi-head attention layer works largely similarly to a single self-attention as above, but the multiple instances of the heads mean that there are multiple instances of the **W<sup>Q</sup>**, **W<sup>K</sup>**, **W<sup>V</sup>** matrices, such that the weights within each set of (**W<sup>Q</sup>**, **W<sup>K</sup>**, **W<sup>V</sup>**) possess different focuses.  
For example, one self-attention head, and therefore one set of (**W<sup>Q</sup>**, **W<sup>K</sup>**, **W<sup>V</sup>**)<sub>1</sub> may be oriented around finding grammatical relations between tokens, whilst a different head with a different set of weights (**W<sup>Q</sup>**, **W<sup>K</sup>**, **W<sup>V</sup>**)<sub>2</sub> may focus on finding tense-based relations between tokens, whilst a different head with a third unique set (**W<sup>Q</sup>**, **W<sup>K</sup>**, **W<sup>V</sup>**)<sub>3</sub> may focus on syntactic relations between tokens. Each set is defined during training (i.e. supervised learning), by choosing relevant loss functions and the training datasets.

As a more specific example, a **Q** matrix in one head may be focusing on whether there are adjectives/adverbs relating to each token within the input sequence. **W<sup>Q</sup>** can be trained with such a focus, via a training dataset in which adjectives and adverbs related to each token are labelled.

Transformers are typically trained end-to-end (all layers trained at once), using specific word-oriented problems. So, before training, the weights within the matrices of a self-attention head \- **W<sup>Q</sup>**, **W<sup>K</sup>**, **W<sup>V</sup>** \- are all initialised at random, then specific training tasks and loss functions are used to refine all parameters within the model, including the weights within each self-attention mechanism. These loss functions are covered in LLM Deep Dive from Springer.

Modern GPUs are able to derive attention scores for one input sequence across multiple heads, simultaneously. This means, for an input sequence of length \(n \), there may be \( O(n^{c}) \) matrices output, where \( c \) is a constant based on the complexity of the Transformer, each matrix signifying the relations between one token and the other tokens within the input sequence.

Consider how abstract and incomprehensible the output of a self-attention head is; computer scientists begin to resemble creative and experimental chefs. This level of abstraction is also a large critique of LLMs generally - difficult to comprehend, therefore difficult to fix when something goes wrong. Significant effort must be put into engineering sanity checks, but this topic is a whole separate chapter in LLM Deep Dive from Springer.
