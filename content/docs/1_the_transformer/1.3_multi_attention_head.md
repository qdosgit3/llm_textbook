---
title: 1.3 Multi-attention Head
type: docs
---

*Finding relations between tokens.*

<img src="/img/transformer-vaswani.png" alt="The Transformer" width="50%"/>

During utilisation, attention regards finding how strongly tokens are interrelated, based on specific types of relations.  
For example, in the context of translation, words cannot be translated word-by-word independently, due to differences in sentence structures across languages, such as English's subject-verb-object pattern compared to the subject-object-verb pattern found in other languages (e.g. Japanese). Instead, the grammatical relations between the words must first be understood.

Mathematically, a single self-attention mechanism head calculates attention scores as follows:  
\(attention(Q, K, V) = softmax(\frac{QKT}{dk} + M) V\)

Note the use of the dot product, which mathematically tends to feature in vector manipulations, because the inputs (tokens) are essentially being represented as vectors (in matrix Q) in potentially a dimension outside of reality (e.g. 2048 dimensions), and then compared to the metadata of the other tokens (in matrix K) also stored as vectors of the same dimension. The result of the dot product will then indicate the best matching token meanings (represented as numbers, in matrix V).

This self-attention mechanism can be broken down into a step-by-step sequence:

1. All the input vectors xi (such as those output from the positional encoding step), and set into a matrix **X**; for example 6 input vectors of dimension 4 would be arranged as a 6x4 matrix.  
     
2. The matrix **X** is multiplied by 3 weight matrices of equivalent dimensions (for the dimensions in this example: 4x4), one matrix **W**Q representing query weights, one matrix **W**K representing key weights, and one matrix **W**V representing value weights, to generate three new matrices **Q**, **K**, and **V**.  
   

| x1 | _ | _ | _ | _ |
| :---- | :---- | :---- | :---- | :---- |
| x2 | _ | _ | _ | _ |
| x3 | _ | _ | _ | _ |
| x4 | _ | _ | _ | _ |
| x5 | _ | _ | _ | _ |
| x6 | _ | _ | _ | _ |

Matrix X above could represent an input such as the sentence "List new ideas for a song", one word per xi row, in the simple case of 4 dimensions per token.

| _ | _ | _ | _ |
| :---- | :---- | :---- | :---- |
| _ | _ | _ | _ |
| _ | _ | _ | _ |
| _ | _ | _ | _ |

*Matrix WK*

| k1 | _ | _ | _ | _ |
| :---- | :---- | :---- | :---- | :---- |
| k2 | _ | _ | _ | _ |
| k3 | _ | _ | _ | _ |
| k4 | _ | _ | _ | _ |
| k5 | _ | _ | _ | _ |
| k6 | _ | _ | _ | _ |

3. The dot product of each query vector qi (a query vector being a row of **Q**) with every key ki via transposed **KT** is calculated, to generate attention scores; QKT.  
   Higher attention scores between a given qi and a given ki indicaŧe that the query qi is more similar to the given ki, and therefore that there is a stronger relation.  
   

| _ | _ | _ | _ | _ | _ |
| :---- | :---- | :---- | :---- | :---- | :---- |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |

*Matrix QKT*

4. These attention scores are then normalised by dividing by the square root of the dimension being used (in this case example, 4); QKTdk.

5. In the decoder, a masking matrix **M** is used to force infinitely negative attention scores between tokens being queried and future tokens, such that only attention scores between a given token and tokens previously generated are used; QKTdk+M.

| _ | _ | _ | _ | _ | _ |
| :---- | :---- | :---- | :---- | :---- | :---- |
| _ | _ | _ | _ | _ | \-∞ |
| _ | _ | _ | _ | \-∞ | \-∞ |
| _ | _ | _ | \-∞ | \-∞ | \-∞ |
| _ | _ | \-∞ | \-∞ | \-∞ | \-∞ |
| _ | \-∞ | \-∞ | \-∞ | \-∞ | \-∞ |

*Matrix … in the case of* 

6. Softmax is run on the attention scores, nullifying the infinitely negative scores, and generating a probability distribution from the attention scores; softmax(QKTdk+M).

| _ | _ | _ | _ | _ | _ |
| :---- | :---- | :---- | :---- | :---- | :---- |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |
| _ | _ | _ | _ | _ | _ |

*After softmax() is applied, in the resulting matrix the cells in each column will add up to 1*

7. The probability distribution is then applied to the values within **V** to find the input tokens with the largest influence on the token under focus.

| v1 | _ | _ | _ | _ |
| :---- | :---- | :---- | :---- | :---- |
| v2 | _ | _ | _ | _ |
| v3 | _ | _ | _ | _ |
| v4 | _ | _ | _ | _ |
| v5 | _ | _ | _ | _ |
| v6 | _ | _ | _ | _ |

| _ | _ | _ | _ |
| :---- | :---- | :---- | :---- |
| _ | _ | _ | _ |
| _ | _ | _ | _ |
| _ | _ | _ | _ |
| _ | _ | _ | _ |
| _ | _ | _ | _ |

*Above: final matrix dimension*

Notes regarding the above example matrices:

* a matrix WK is of size 4x4 means that there are 4 keys stored within the self-attention mechanism, each with a level of detail of dimension 4  
* approximately all the cells have been left blank, because although every cell would contain a number in a working implementation, the numbers hold no meaning to humans, and so they have been omitted for readability  
* in modern LLM implementations, the dimensions of the key, value, and initial inputs to the Transformer may differ in dimensionality

More algebraic, implicit examples of the matrix transformations are available at:  
https://www.columbia.edu/\~jsl2239/transformers.html  
https://web.stanford.edu/\~jurafsky/slp3/9.pdf

The multi-head attention layer works largely similarly to a single self-attention as above, but the multiple instances of the heads mean that there are multiple instances of the **W**Q, **W**K, and **W**V matrices, such that the weights within each set of (WQ, WK, WV) possess different focuses.  
For example, one self-attention head may be oriented around finding grammatical relations between tokens, a different head may focus on finding tense-based relations between tokens, whilst a different head may focus on syntactic relations between tokens.  
As a more specific example, a **Q** matrix in one head may be focusing on whether there are adjectives/adverbs relating to each token within the input sequence (presuming **W**Q was trained with such a focus).

Transformers are typically trained end-to-end (all layers trained at once), using specific word-oriented problems. So, before training, the weights within the matrices of a self-attention head \- **W**Q, **W**K, **W**V \- all initialised at random, then specific training tasks and loss functions are used to refine all parameters within the model, including the weights within each self-attention mechanism. These loss functions are covered in LLM Deep Dive from Springer.

Modern GPUs are able to derive attention scores for one input sequence across multiple heads, simultaneously. This means, for an input sequence n, there may be O(n2) matrices output, each matrix signifying the relations between one token and the other tokens within the input sequence.
