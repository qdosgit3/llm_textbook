---
title: 1.1 Input Embeddings
type: docs
---

*Converting natural language to numerical representations.*

<img src="/img/transformer-vaswani.png" alt="The Transformer" width="50%"/>

*Diagram 1: The Transformer, Vaswani et al. (2017)*

The inputs to the encoder are first split into tokens. A token may consist of a whole word, or a portion of a word.  
These tokens are then mapped to vector representations (a numerical representation of a token). An example of a vector representation would be a word embedding. A word embedding represents tokens in a dense way, such that similar tokens show high cosine similarity when compared (in vector representation). This dense representation also allows models to comprehend a large quantity of different tokens.  


Word embeddings may be developed via:
* neural networks and large quantities of unlabeled training data, by optimising a loss function based on tokens that are expected to be close together (e.g. Word2Vec from Google, 2013\)  
* global co-occurrence statistics and subword information (e.g. GloVe from Pennington et al. 2014\)

Modern word embeddings are typically of very high dimension, e.g. Meta's LLM Llama-3 uses 4096 dimensions for larger models.  
An example of a 16-dimension word embedding, which represents the word 'book', could be:

| 0.02 | 0.15 | 0.31 | 0.08 | \-0.26 | 0.21 | \-0.58 | 1.12 | \-0.38 | \-0.91 | 0.52 | 0.87 | \-0.17 | 0.73 | \-0.38 | 0.18 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |

Mapping a token to a word embedding is generally a deterministic process, including in the context of Llama-3.

Representing data as vectors is a method that can be applied to a variety of contexts, and so is independently a large domain even outside the context of LLMs. For example, representations may be utilised to find similarities between documents, for classification or information retrieval purposes.
