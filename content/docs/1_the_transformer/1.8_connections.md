---
title: 1.8 Connections
type: docs
---

*Wirings between Transformer inputs and outputs.*

<img src="/img/transformer-vaswani.png" alt="The Transformer" width="50%"/>

*Diagram 1.0: The Transformer, Vaswani et al. (2017)*

An initial glimpse of the Transformer may raise questions regarding why both encoder and decoder have inputs. This is not made entirely clear in the original Vaswani et al. (2017) paper. The following diagrams depict why \- the decoder blocks take as input the encoded initial input sequence, as well as the generated tokens that the Transformer itself has generated, and continues to feed newly generated tokens back into the decoder until the output sequence is finished.

![Diagram depicting connections between stacked encoder blocks, stacked decoder blocks, and outputs and inputs between each](/img/encoder-decoder-connections-1.png)
*Diagram 1.8.1: an LLM performing a machine translation task, note that this model features stacked encoders and decoders, not uncommonly. Source: [HuggingFace blog](https://huggingface.co/blog/encoder-decoder)*

![Animation of input sequence and previously generated tokens going into a black box, and generating the next token, and repeating until all tokens have been generated.](/img/decoder-gen.gif)

*Diagram 1.8.2: an LLM performing a chatbot style task, to depict, with the Transformer as a black box, how newly generated tokens are fed as input back into the Transformer. Source: [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8)*

