---
title: 1.8 Connections
type: docs
---

*Wirings between Transformer inputs and outputs.*

<img src="/img/transformer-vaswani.png" alt="The Transformer" width="50%"/>

*Diagram 1: The Transformer, Vaswani et al. (2017)*

An initial glimpse of the Transformer may raise questions regarding why both encoder and decoder have inputs. This is not made entirely clear in the original Vaswani et al. (2017) paper. The following diagrams depict why \- the decoder blocks take as input the encoded initial input sequence, as well as the generated tokens that the Transformer itself has generated.

![Diagram depicting connections between stacked encoder blocks, stacked decoder blocks, and outputs and inputs between each](/img/encoder-decoder-connections-1.png)
Diagram 1.5: 

Source: [HuggingFace blog](https://huggingface.co/blog/encoder-decoder)

![Animation of input sequence and previously generated tokens going into a black box, and generating the next token, and repeating until all tokens have been generated.](/img/decoder-gen.gif)

Source: [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8)

