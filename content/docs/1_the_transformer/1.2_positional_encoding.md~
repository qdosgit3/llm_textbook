---
title: 1.2 Positional Encoding
type: docs
---

*Incorporating position related data to word embeddings.*


<img src="/img/transformer-vaswani.png" alt="The Transformer" width="50%"/>

*Diagram 1: The Transformer, Vaswani et al. (2017)*

As a Transformer based LLM processes the aforementioned tokens in parallel, a method is needed to embed the relative positional data of the tokens into vector representations. The original Vaswani et al. (2017) paper proposes using the sine and cosine functions as a means of representing positions, alternately for odd and even positions.

Positional encodings are then generated via the following functions:  
\(P(k, 2i) = sin(\frac{k}{n^{2i/d}})  \)\
\(P(k, 2i+1) = cos(\frac{k}{n^{2i/d}}) \)

k: the position of the object within the input sequence

d: set to the same value as the dimension of the word embeddings to be used

n: a constant called the scalar, set to 10,000 in Vaswani et al. (2017)

i: the output position in regards to the final positional encoding vector that is to be output for a specific token, whereby each set of adjacent odd and even position values are set to the same i, such that \(0 \le i \le \frac{d}{2} \)

So, for example, generating 16-dimension positional encodings for the input sequence "classify the book", may look like the following, with n set to 100:

| i | 0 | 0 | 1 | 1 | 2 | 2 | 3 | 3 | 4 | 4 | 5 | 5 | 6 | 6 | 7 | 7 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **classify**<br/> k \= 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 |
| **the**<br/> k \= 1 | 0.84 | 0.54 | 0.53 | 0.85 | 0.31 | 0.95 | 0.18 | 0.98 | 0.10 | 1.00 | 0.06 | 1.00 | 0.03 | 1.00 | 0.02 | 1.00 |
| **book**<br/> k \= 2 | 0.91 | \-0.42 | 0.90 | 0.41 | 0.59 | 0.81 | 0.35 | 0.94 | 0.20 | 0.98 | 0.11 | 0.99 | 0.06 | 1.00 | 0.04 | 1.00 |

These positional encodings are then added to the word embeddings, for example:

| 0.02 | 0.15 | 0.31 | 0.08 | \-0.26 | 0.21 | \-0.58 | 1.12 | \-0.38 | \-0.91 | 0.52 | 0.87 | \-0.17 | 0.73 | \-0.38 | 0.18 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |

\+

| 0.91 | \-0.42 | 0.90 | 0.41 | 0.59 | 0.81 | 0.35 | 0.94 | 0.20 | 0.98 | 0.11 | 0.99 | 0.06 | 1.00 | 0.04 | 1.00 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |

\=

| 0.93 | \-0.27 | \-0.59 | \-.033 | \-0.85 | \-0.6 | \-0.93 | 0.18 | \-0.58 | \-1.89 | 0.41 | \-0.12 | \-0.23 | \-0.27 | \-0.42 | \-0.82 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |

This final result, above, is then input into the encoder of the Transformer.

The reasons that positions are encoded via sinusoidal functions are as follows:

* Sinusoidal functions allow a relative position encoding P+K to derived given the encoding for a specific position P  
* Sinusoidal functions of alternating wavelengths map relatively closely to binary numbers (see diagram 2, 3), therefore allowing uniqueness (reductionistically, binary numbers are the simplest unique representation)  
* Sinusoidal functions are of infinite length, allowing them to be adapted for any input sequence length  
* Sinusoidal functions can maintain an output within a limited range, that does not introduce issues with gradient descent during optimisation via loss function  
* Sinusoidal functions can be quickly computed, in a deterministic way, allowing efficiency and dependability

The means that a relative position can be derived from a specific position is via the standard 3D rotational matrix; a full proof is available on huggingface.co, see [chapter sources](/docs/the-transformer/).

<br/>

![150 in binary, graphically depicted](/img/150-binary.png)  
*Diagram 2: sourced from HuggingFace, representing the number 150 in binary format, across 8 dimensions.*
<br/>
<br/>

![150 as a positional encoding, graphically depicted](/img/150-sine-cosine.png)  
*Diagram 3: sourced from HuggingFace, attempts to depict the positional encoding where \(k = 150 \), \(n = 10000\), \(d = 8 \), where each 2 rows represent a different value for \(i\), starting from \(i = 0\) at the top.*

Note that there is seemingly a bug in the animation that diagram 3 is sourced from, and, as such, the numbers to the left are wrong. Nonetheless, concepts such as minimalistic uniqueness, infiniteness, and limitation of range, should be apparent, when comparing the sinosoidal functions to the binary representation of an equivalent dimension.
